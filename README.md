<div align = "center">
    <image src = "./pics/logo.jpg" width = "50%"/>
	<h1 align = "center">基于机器学习的分布式系统故障诊断系统</h1>    
    <h4 align = "center">系统治愈师团队 | System Healer Team</h4>
</div>
## 一、信息介绍

大数据时代，分布式系统成为信息存储和处理的主流系统。相对于传统系统而言，分布式系统更为庞大和复杂，故障发生的平均几率比较高，其运维的难度和复杂度大大提高。如何对分布式系统进行高效、准确的运维，成为保障信息系统高效、可靠运行的关键问题。
为此，我们团队设计了故障诊断模型，可以高效地分析并识别故障类别，实现分布式系统故障运维的智能化，快速恢复故障的同时大大降低分布式系统运维工作的难度，减少运维对人力资源的消耗。

在分布式系统中某个节点发生故障时，故障会沿着分布式系统的拓扑结构进行传播，造成自身节点及其邻接节点相关的KPI指标和发生大量日志异常。中兴通讯为我们提供分布式数据库的故障特征数据和标签数据，其中特征数据是系统发生故障时的KPI指标数据，KPI指标包括由feature0、feature1 ...feature106共107个指标，标签数据为故障类别数据，共6个类别，用0、1、2、3、4、5分别表示6个故障，根据这些数据，我们借助机器学习、深度学习、web等技术搭建故障诊断系统，该系统支持用户上传训练集对模型进行训练和模型下载，同时支持用户上传单条或多条测试语句进行测试并可视化测试结果，同支持测试结果与模型的下载。

我们的项目采用`Django + BootStrap`为框架进行web系统的开发，模型采用`多层感知机（MLP）模型`，它是一种用于处理监督学习问题的神经网络，是一种前馈神经网络，在处理此类问题上具有不俗的表现。

下面是我们团队的分工：

| 团队成员 | 分工                                               | 备注 |
| -------- | -------------------------------------------------- | ---- |
| 张骁凯   | 负责django web框架的搭建，测试即Web服务器的部署    | 队长 |
| 杨尚君   | 负责数据预处理部的编写、测试，前端页面的设计与实现 | 队员 |
| 李思成   | 负责深度学习模型的搭建，测试，参数调整和数据可视化 | 队员 |

## 二、Web系统介绍

### 2.1 概述

![index_page](./pics/website.png)



### 2.2 部署方法

####  2.2.1 使用脚本进行部署

在`SystemHealer/`目录运行`setup.sh`脚本，可以一键构建环境，安装有关依赖，并打开默认浏览器预览网页。

如果系统没有打开浏览器，可以在构建完成之后手动输入<127.0.0.1:8000>或者<localhost:8000>来访问web系统。

#### 2.2.2 使用 Docker 进行部署

在`SystemHealer/`目录运行以下命令来构建 Docker 镜像：
```bash
docker build -t systemhealer .
```

构建完成后，运行以下命令来启动 Docker 容器：

```bash
docker run -p 8000:8000 systemhealer
```

现在，您可以在浏览器中输入<127.0.0.1:8000>或 `localhost:8000` 来访我们的Web系统。

## 三、模型介绍

我们的项目采用`多层感知机（MLP）模型`，它是一种用于处理监督学习问题的神经网络，是一种前馈神经网络，在处理此类问题上具有不俗的表现。下面是我们项目中MP的定义：
```python
class MLP(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(MLP, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, hidden_size)
        self.fc4 = nn.Linear(hidden_size, num_classes)
        self.activation = nn.LeakyReLU()

    def forward(self, x):
        out = self.fc1(x)
        out = self.activation(out)
        out = self.fc2(out)
        out = self.activation(out)
        out = self.fc3(out)
        out = self.activation(out)
        out = self.fc4(out)
        return out
```

在这个模型中，我们有一个输入层，两个隐藏层，以及一个输出层。首先，我们定义了一个名为` MLP `的类，它继承了` PyTorch` 的 `nn.Module` 类。`nn.Module `是所有神经网络模块的基类，我们自定义的模型也继承这个类。

在` MLP `类的构造函数中，我们定义了四个全连接层（也称为线性层）。每个全连接层都使用` nn.Linear `类创建，该类在输入数据上应用线性变换。第一个全连接层（`fc1`）的输入大小为` input_size`，输出大小为` hidden_size`。接下来的两个全连接层（`fc2 `和` fc3`）的输入和输出大小都是`hidden_size`。最后一个全连接层（`fc4`）的输入大小为` hidden_size`，输出大小为 `num_classes`，这表示我们的模型将输出 `num_classes `个类别的预测。

我们还定义了一个激活函数，使用了 `nn.LeakyReLU` 类。`LeakyReLU` 是修正线性单元（`ReLU`）的一个变种，它解决了 `ReLU` 的`“死亡 ReLU”`问题，即一些神经元可能会停止学习，因为它们进入了一个永远不会被激活的状态。

在前向传播函数（forward）中，我们将输入数据 x 通过每个全连接层和激活函数，最后输出结果。注意，我们没有在最后一个全连接层后应用激活函数，这是因为我们通常会在计算损失函数时使用包含激活函数的损失函数，例如` nn.CrossEntropyLoss`。

使用多层感知机模型有以下优点和特点：

1. **处理监督学习问题：** 多层感知机（`MLP`）是一种用于处理监督学习问题的神经网络模型。监督学习是一种机器学习任务，其中模型从带有标签的训练数据中学习，并预测未标记数据的标签。`MLP`通过前向传播算法和反向传播算法来学习输入数据与标签之间的关系。
2. **前馈神经网络：** `MLP`是一种前馈神经网络，这意味着数据在网络中沿一个方向流动，不会形成循环连接。这种结构使得`MLP`可以对输入数据进行逐层处理，每一层都产生中间的表示，最终输出预测结果。
3. **多层结构**：` MLP`包含多个层次的神经元，通常包括输入层、隐藏层和输出层。隐藏层是位于输入层和输出层之间的层次，通过这些层次可以学习更加复杂和抽象的特征表示。在给定足够的隐藏单元和适当的参数设置下，`MLP`可以学习到非线性关系，从而具备更强的模型表达能力。
4. **全连接层：** `MLP`的每个层都是全连接层，也称为线性层。全连接层中的神经元与上一层的所有神经元都有连接，这样可以实现输入数据和权重的线性组合，并通过激活函数引入非线性。
5. **激活函数：** 在`MLP`中，我们使用了`LeakyReLU`作为激活函数。激活函数的作用是引入非线性，使得模型可以学习非线性关系。`LeakyReLU`是修正线性单元（`ReLU`）的一个改进版本，当输入小于零时，它引入一个小的斜率，避免了`ReLU`的“死亡神经元”问题。
6. **模型训练：** `MLP`通过前向传播和反向传播算法进行模型训练。前向传播是将输入数据通过网络的每一层，得到最终的预测结果。反向传播通过计算预测结果与真实标签之间的误差，并沿着网络的方向更新权重，使得模型能够逐渐减小误差，提高预测的准确性。
7. **模型应用：** `MLP`模型在分类和回归等监督学习任务中表现出色。它可以处理各种类型的输入数据，例如图像、文本、声音等。`MLP`的强大表达能力和灵活性使得它成为机器学习和深度学习领域的重要工具之一。

总之，`MLP`模型是一种灵活且强大的神经网络模型，适用于处理各种监督学习问题。它的多层结构和全连接层使其能够学习非线性模型，并通过适当的调整隐藏层的数量和大小来控制模型的复杂性。在我们的项目中，采用MLP模型可以有效处理分布式故障诊断等任务，并获得优秀的结果。

## 四、实现思路

### 4.1 数据预处理

在我们的故障诊断系统中，数据预处理是一个重要的步骤。我们需要确保输入到模型的数据是完整的，没有缺失值，并且格式正确。下面是我们进行数据预处理的主要步骤：

#### 4.1.1 去除重复值

首先，我们使用`drop_duplicates`函数去除数据中的重复值。这是一个重要的步骤，因为重复的数据点可能会导致模型过拟合，即模型过于复杂，以至于它开始记忆训练数据，而不是从中学习模式。当我们的数据集中存在重复的样本时，模型在训练过程中可能会过度关注这些重复样本，导致对它们的预测过于自信，而对其他样本的泛化能力下降。

重复数据的存在可能是由于多种原因引起的，例如数据收集过程中的误操作、系统错误或者数据来源的特殊性。无论是哪种情况，重复数据都可能对我们的分析和建模过程产生负面影响，因此需要及时处理。

通过调用`drop_duplicates`函数并将参数`inplace`设置为`True`，我们可以直接在原始数据上进行修改，而无需创建新的数据副本。这样做不仅可以减少内存的使用，还可以提高代码的执行效率。

`drop_duplicates`函数会比较数据集中的每个数据点，并检查其与其他数据点是否完全相同。如果找到了重复的数据点，函数会将其删除，只保留一个副本。这样，我们就能确保数据集中的每个样本都是唯一的，避免了冗余数据的干扰。

通过去除重复数据，我们可以消除数据集中的冗余信息，提高模型的训练效果。当模型面临更清晰、更准确的数据时，它可以更好地学习数据中的模式和规律。这将有助于改善模型的泛化能力，使其能够更好地应用于未见过的数据，从而提高预测和决策的准确性。

总之，去除重复数据是数据预处理的重要一步，它有助于净化数据集，提高模型的性能和鲁棒性。在进行任何进一步的分析和建模之前，我们应该始终注意并处理数据中的重复值，以确保我们所使用的数据是可靠、准确且唯一的。

```python
def preprocess(data):
    # 去掉重复值
    data.drop_duplicates(inplace=True)
```

#### 4.1.2 缺失值处理

然后，我们使用`IterativeImputer`类来填充数据中的缺失值。`IterativeImputer`是一种高级的填充方法，它通过迭代的方式使用其他特征的值来预测缺失值，并在每次迭代中更新缺失值的预测结果。这种迭代的过程可以提高填充的准确性和可靠性。

在代码中，我们创建了一个名为`imputer`的`IterativeImputer`对象，并设置了`random_state`为0，以确保每次运行时得到相同的结果。接下来，我们使用`imputer.fit_transform(data)`方法对数据进行拟合和转换操作，其中`data`是待填充的数据集。

在拟合的过程中，`IterativeImputer`会首先对数据集中的特征进行分析，并建立一个预测模型来预测缺失值。然后，它使用已有的非缺失值特征作为输入，利用建立的模型来预测缺失值。这个过程会不断迭代，直到达到收敛条件或者达到预先设定的最大迭代次数。

转换的结果是一个新的数据集data_imputed，它是一个`DataFrame`对象，具有与原始数据相同的列名。这个新数据集中的缺失值已经被填充，不再存在。为了保持一致性，我们将`data_imputed`的列名设置为与原始数据相同。

接着，我们通过`data_imputed.dropna()`方法去除仍然存在缺失值的行，以保证数据的完整性和一致性。然后，我们将数据集中的目标变量（标签）提取出来，存储在y变量中，以备后续的建模和分析使用。

最后，根据需求，我们定义了一个列表`columns_to_drop`，其中包含需要从数据集中删除的特征列名。通过调用`data_imputed.drop(columns_to_drop, axis=1)`，我们可以删除这些特征列，以满足后续分析的需要。经过这一步操作后，`data_imputed`将只包含用于建模的特征列，不再包含标签列。

总结而言，我们进行了一次完整的数据预处理流程，包括使用`IterativeImpute`类进行缺失值填充，删除含有缺失值的行，提取标签列，并根据需求删除特定的特征列。通过这些处理步骤，我们能够准备好适用于机器学习模型训练和分析的数据集。

```python
imputer = IterativeImputer(random_state=0)
data_imputed = imputer.fit_transform(data)
data_imputed = pd.DataFrame(data_imputed, columns=data.columns)
data_imputed = data_imputed.dropna()
y = data_imputed['label']
columns_to_drop = ['label']
data_imputed = data_imputed.drop(columns_to_drop, axis=1)
```

#### 4.1.3 标准化处理

最后，我们使用`StandardScaler`类对数据进行标准化。标准化是一种常见的预处理步骤，它可以将所有特征的值调整到同一尺度，通常是均值为0，标准差为1。这个过程涉及两个主要步骤：计算每个特征的均值和标准差，然后将每个特征的值减去均值并除以标准差。

首先，我们创建了一个名为scaler的`StandardScaler`对象。这个对象将用于计算均值和标准差，并进行标准化转换。然后，我们使用`fit_transform`方法将数据集`data_imputed`传递给`scaler`对象进行处理。`fit_transform`方法执行了两个步骤：首先，它通过计算每个特征的均值和标准差来学习数据集的统计信息，然后将数据集进行标准化转换。

标准化的目的是消除不同特征之间的量纲差异，确保它们都在相同的尺度上进行比较。在机器学习中，很多算法都对特征的尺度敏感。如果特征具有不同的尺度，某些特征的值范围可能会主导算法的训练过程，导致其他特征的影响力被忽略。通过将所有特征标准化到相同的尺度，我们可以确保每个特征对算法的影响程度是均等的。

标准化还有助于提高算法的收敛速度和稳定性。某些优化算法（如梯度下降）在处理标准化的数据时更容易找到最优解。此外，标准化还可以提高特征的解释性。当特征处于相同的尺度上时，我们可以更直观地理解它们对目标变量的影响。

需要注意的是，标准化是在训练数据上进行的，并且使用同一套参数对测试数据进行相同的标准化处理。这是为了保持训练集和测试集之间的一致性。在实际应用中，我们通常会将整个数据集划分为训练集和测试集，并且只使用训练集的统计信息来进行标准化，以避免测试集信息泄露。

总之，通过使用`StandardScaler`类对数据进行标准化，我们可以消除特征之间的尺度差异，提高机器学习算法的效果，并增强特征的解释能力。这是一个在数据预处理中常用且重要的步骤，为我们提供了更可靠和一致的分析结果。

```python
scaler = StandardScaler()
data_standerd = scaler.fit_transform(data_imputed)
return data_standerd, y
```

### 4.2 过采样处理

然后，我们使用`KMeansSMOTE`类进行过采样。`KMeansSMOTE`是一种用于处理不平衡数据集的技术，它结合了`K-Means`聚类和`SMOTE`（合成少数类过采样技术）来生成新的少数类样本。`K-Means`聚类是一种常见的无监督学习算法，通过将数据分为`K`个簇来寻找数据的内在结构。它根据数据的特征相似性将样本分组，将相似的样本归为同一簇，从而在数据中发现隐藏的模式和结构。

在我们的情况中，`KMeansSMOTE`利用`K-Means`聚类算法对数据集进行聚类，将样本分成不同的簇。然后，针对少数类样本较少的簇，我们使用`SMOTE`技术生成合成样本。`SMOTE`（`Synthetic Minority Over-sampling Technique`）是一种经典的过采样技术，它通过插值生成新的合成少数类样本，以增加少数类的样本数量。

`KMeansSMOTE`的主要优势在于它结合了两种方法的优点：`K-Means`聚类可以帮助我们理解数据集的结构和模式，而`SMOTE`可以增加样本数量来平衡类别不平衡问题。通过将这两种技术结合起来，`KMeansSMOTE`可以生成更加合理和真实的合成样本，有助于改善模型的性能和泛化能力。

通过使用`KMeansSMOTE`进行过采样，我们能够解决数据集中存在的类别不平衡问题。类别不平衡可能导致模型对多数类样本进行过度拟合，从而忽略少数类样本。通过生成合成样本，我们可以增加少数类样本的数量，使得模型能够更好地学习并识别少数类的特征和模式。

总结而言，`KMeansSMOTE`是一种结合了`K-Means`聚类和`SMOTE`技术的过采样方法，适用于处理不平衡数据集。它通过聚类分析和合成样本生成，帮助我们处理类别不平衡问题，提高模型性能和泛化能力。这种技术在许多实际应用中被广泛使用，特别是在医疗诊断、金融风险预测和欺诈检测等领域，以处理不平衡数据集并改善模型的预测能力。

```python
kmeans_smote = KMeansSMOTE(random_state=42)
X_train, y_train = kmeans_smote.fit_resample(X_train, y_train)
```

## 2. 采样与类别权重：

考虑到类别之间可能存在不平衡的情况，我们使用`compute_class_weight`计算每个类别的权重，并将其用于损失函数的计算中，以减轻类别不平衡带来的影响。

## 3. 模型构建：

使用ResNet18作为基本模型，输入维度为107（特征数量），输出维度为6（故障类别数量）。在ResNet18中，我们使用1D卷积操作来处理输入的特征数据，并在每个残差块之间添加Dropout层以提高模型的泛化能力。

## 4. 损失函数与优化器：

使用交叉熵损失（CrossEntropyLoss）作为损失函数，其中加入了类别权重。使用Adam优化器进行模型参数的更新。

## 5. 训练与验证：

在每个训练周期（epoch）中，对训练数据进行随机打乱，然后按批次进行训练。在每个批次中，先进行前向传播计算输出，然后计算损失并进行反向传播更新参数。在每个训练周期结束后，对验证集进行评估，计算验证集上的准确率并保存模型。

## 五、原理分析



## 六、结果分析

t-SNE（t-Distributed Stochastic Neighbor Embedding）是一种用于数据可视化的机器学习算法。它是一种降维技术，特别适合用于高维数据的可视化。t-SNE的主要目标是在低维空间（通常是二维或三维）中保留高维空间中的相似性结构。

以下是对t-SNE的基本介绍：

1. **工作原理**：t-SNE首先在高维空间中计算数据点之间的相似度，然后在低维空间中创建一个概率分布，使得低维空间中的点的相似度尽可能接近高维空间中的相似度。这样，高维空间中相似的点在低维空间中也会相似。
2. **优点**：t-SNE相比于其他降维方法（如PCA）的优点在于，它能更好地保留局部结构。这意味着在高维空间中相互靠近的点在低维空间中也会靠近。这使得t-SNE非常适合用于数据可视化。
3. **缺点**：t-SNE的一个主要缺点是它可能不会很好地保留全局结构。也就是说，高维空间中远离的点在低维空间中可能不会远离。此外，t-SNE的结果也可能对初始参数（如随机种子）敏感。
4. **应用**：t-SNE广泛应用于各种领域，包括机器学习、数据挖掘、信息检索和生物信息学等。它常常用于探索性数据分析，以帮助研究人员理解高维数据的结构。
5. **算法步骤**：
   - 在高维空间中计算点之间的条件概率，使得相似的对象有更高的概率被选中。
   - 在低维空间中定义相似的条件概率，并最小化两个条件概率分布的KL散度。
   - 通过梯度下降等优化方法，找到最佳的低维空间表示。

以上是对t-SNE的基本介绍，如果你需要更详细的信息或者具体的使用方法，我可以提供更多的帮助。

## 部署

该 Web 系统通过使用 Docker 和 Nginx 进行部署, 从而实现了以下特性:

1. 容器化部署：

使用 Docker 容器化技术，将整个 Web 系统打包为独立的容器。

2. 高可用性和负载均衡：

Nginx 作为反向代理服务器，提供高可用性和负载均衡功能。使用 Nginx 进行请求的负载均衡。

3. 静态资源缓存和压缩：

配置 Nginx 以缓存常用的静态文件，如 CSS、JavaScript 和图像，减少对后端服务器的请求，提高响应速度。
